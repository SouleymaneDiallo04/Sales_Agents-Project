# ğŸ“Š RÃ©sultats du fine-tuning â€“ Agent de Pricing RL (PPO)

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ§  CONTEXTE ET OBJECTIF DU FINE-TUNING                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ce qui a Ã©tÃ© rÃ©alisÃ© ici correspond Ã  un **fine-tuning dâ€™un agent de Reinforcement Learning**, basÃ© sur lâ€™algorithme **PPO (Proximal Policy Optimization)**, dont lâ€™objectif est **dâ€™apprendre une stratÃ©gie optimale de pricing** pour un produit donnÃ©.

Le produit concernÃ© est :
- **Product ID : `PROD_001`**
- **Nom : Smartphone Galaxy X**

Lâ€™agent apprend Ã  ajuster les dÃ©cisions de prix Ã  partir dâ€™un **environnement de simulation personnalisÃ©**, connectÃ© Ã  une **base de donnÃ©es MySQL**, contenant les informations nÃ©cessaires Ã  lâ€™Ã©valuation des actions (rÃ©compenses).

Lâ€™entraÃ®nement a Ã©tÃ© exÃ©cutÃ© **sur CPU**, Ã  lâ€™aide de **Stable-Baselines3**, avec **TensorFlow (oneDNN activÃ©)** comme backend de calcul.

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš™ï¸ CONTEXTE Dâ€™EXÃ‰CUTION                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Ã‰lÃ©ment | Valeur |
|------|------|
| Date / Heure | 2026-01-10 (07:20 â†’ 07:25) |
| Device | CPU |
| Framework RL | Stable-Baselines3 â€“ PPO |
| Backend | TensorFlow (oneDNN activÃ©) |
| Base de donnÃ©es | MySQL (`rl_data_base`) |
| Cache | Redis dÃ©sactivÃ© |

> â„¹ï¸ oneDNN activÃ© â†’ lÃ©gÃ¨res variations numÃ©riques possibles.

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“¦ PRODUIT CONCERNÃ‰                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Champ | Valeur |
|------|------|
| Product ID | `PROD_001` |
| Nom produit | Smartphone Galaxy X |
| Prix actuel | N/A |
| Stock | N/A |

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“ CONFIGURATION GÃ‰NÃ‰RALE DE Lâ€™ENTRAÃNEMENT                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| ParamÃ¨tre | Valeur |
|---------|--------|
| Algorithme | PPO |
| Learning rate | 0.0003 |
| Clip range | 0.2 |
| Total timesteps demandÃ©s | 50 000 |
| Total timesteps effectuÃ©s | 51 200 |
| Environnement | Custom Pricing Environment |

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“ˆ DÃ‰ROULEMENT ET PROGRESSION DE Lâ€™APPRENTISSAGE              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Le processus dâ€™entraÃ®nement sâ€™est dÃ©roulÃ© **de maniÃ¨re rÃ©guliÃ¨re et stable** jusquâ€™Ã  environ **50 000 timesteps**.  
On observe une **augmentation progressive du reward maximal**, ce qui traduit une amÃ©lioration continue de la politique apprise par lâ€™agent.

Le reward passe :
- dâ€™environ **38.9** lors des premiÃ¨res itÃ©rations,
- Ã  une valeur finale proche de **75.11**.

Cela indique que lâ€™agent apprend effectivement **une stratÃ©gie de pricing de plus en plus efficace**, maximisant la rÃ©compense dÃ©finie par lâ€™environnement.

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ† Ã‰VOLUTION DES MEILLEURS REWARDS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| ItÃ©ration | Timesteps | Meilleur reward |
|---------|-----------|----------------|
| 1 | 2 048 | 38.90 |
| 2 | 4 096 | 57.00 |
| 7 | 14 336 | 62.19 |
| 8 | 16 384 | 62.82 |
| 10 | 20 480 | 63.07 |
| 13 | 26 624 | 70.84 |
| 14 | 28 672 | 72.65 |
| 15 | 30 720 | 74.25 |
| 18 | 36 864 | 74.66 |
| 22 | 45 056 | **75.11** |

â¡ï¸ **Reward maximal atteint : 75.11**

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š ANALYSE DES MÃ‰TRIQUES PPO ET CONVERGENCE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Les mÃ©triques internes du modÃ¨le PPO confirment une **trÃ¨s bonne convergence** :

- **Explained variance â‰ˆ 0.999**  
  â†’ la fonction de valeur explique presque parfaitement la variance des rÃ©compenses.

- **Value loss faible**  
  â†’ les estimations de la valeur dâ€™Ã©tat sont prÃ©cises.

- **Policy gradient loss stable**  
  â†’ les mises Ã  jour de la politique sont maÃ®trisÃ©es, sans oscillations.

- **Entropy loss dÃ©croissante**  
  â†’ la politique devient progressivement **plus dÃ©terministe**, signe que lâ€™agent est confiant dans ses dÃ©cisions.

Ces Ã©lÃ©ments indiquent que le modÃ¨le a **convergÃ© de maniÃ¨re saine**, sans instabilitÃ© ni effondrement de politique, et que le fine-tuning peut Ãªtre considÃ©rÃ© comme **rÃ©ussi du point de vue apprentissage**.

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¾ SAUVEGARDE DU MODÃˆLE                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Le modÃ¨le entraÃ®nÃ© a Ã©tÃ© correctement sauvegardÃ© sur disque :

